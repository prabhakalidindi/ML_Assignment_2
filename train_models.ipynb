{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwtwg3BTuhs_",
        "outputId": "25912e36-61cc-4097-c335-ce58545dc86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample test data saved.\n",
            "Training models... (This may take a minute)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Logistic Regression completed.\n",
            "✔ Decision Tree completed.\n",
            "✔ KNN completed.\n",
            "✔ Naive Bayes completed.\n",
            "✔ Random Forest completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [15:04:36] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ XGBoost completed.\n",
            "\n",
            "=== Copy this table to your README ===\n",
            "| Model               |   Accuracy |   AUC |   Precision |   Recall |    F1 |   MCC |\n",
            "|:--------------------|-----------:|------:|------------:|---------:|------:|------:|\n",
            "| Logistic Regression |      0.927 | 0.994 |       0.928 |    0.927 | 0.927 | 0.912 |\n",
            "| Decision Tree       |      0.89  | 0.933 |       0.891 |    0.89  | 0.89  | 0.868 |\n",
            "| KNN                 |      0.923 | 0.982 |       0.924 |    0.923 | 0.924 | 0.908 |\n",
            "| Naive Bayes         |      0.758 | 0.962 |       0.756 |    0.758 | 0.756 | 0.709 |\n",
            "| Random Forest       |      0.925 | 0.993 |       0.926 |    0.925 | 0.926 | 0.91  |\n",
            "| XGBoost             |      0.924 | 0.994 |       0.925 |    0.924 | 0.925 | 0.909 |\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "import joblib\n",
        "\n",
        "# 1. Setup\n",
        "if not os.path.exists('model'):\n",
        "    os.makedirs('model')\n",
        "\n",
        "# 2. Load Data\n",
        "# Ensure the file name matches what you downloaded\n",
        "if os.path.exists('Dry_Bean_Dataset.csv'):\n",
        "    df = pd.read_csv('Dry_Bean_Dataset.csv')\n",
        "elif os.path.exists('Dry_Bean.csv'):\n",
        "    df = pd.read_csv('Dry_Bean.csv')\n",
        "else:\n",
        "    print(\"Error: Dataset CSV not found. Please download it from Kaggle.\")\n",
        "    exit()\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Encode Target (Strings -> Numbers)\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "joblib.dump(le, 'model/label_encoder.pkl') # Save for app\n",
        "\n",
        "# Split (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "joblib.dump(scaler, 'model/scaler.pkl')\n",
        "\n",
        "# Save Sample Test Data (with original labels for user clarity)\n",
        "test_df = X_test.copy()\n",
        "test_df['Class'] = le.inverse_transform(y_test)\n",
        "test_df.to_csv('sample_test_data.csv', index=False)\n",
        "print(\"Sample test data saved.\")\n",
        "\n",
        "# 3. Define Models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=3000, multi_class='multinomial'),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', objective='multi:softprob')\n",
        "}\n",
        "\n",
        "# 4. Train & Evaluate\n",
        "results = []\n",
        "print(\"Training models... (This may take a minute)\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Use scaled data for Logistic/KNN, raw for others (optional, but good practice)\n",
        "    if name in [\"Logistic Regression\", \"KNN\"]:\n",
        "        X_t, X_v = X_train_scaled, X_test_scaled\n",
        "    else:\n",
        "        X_t, X_v = X_train, X_test\n",
        "\n",
        "    model.fit(X_t, y_train)\n",
        "    preds = model.predict(X_v)\n",
        "    probs = model.predict_proba(X_v)\n",
        "\n",
        "    # Calculate Metrics (Weighted for Multi-Class)\n",
        "    metrics = {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, preds),\n",
        "        \"AUC\": roc_auc_score(y_test, probs, multi_class='ovr', average='weighted'),\n",
        "        \"Precision\": precision_score(y_test, preds, average='weighted'),\n",
        "        \"Recall\": recall_score(y_test, preds, average='weighted'),\n",
        "        \"F1\": f1_score(y_test, preds, average='weighted'),\n",
        "        \"MCC\": matthews_corrcoef(y_test, preds)\n",
        "    }\n",
        "    results.append(metrics)\n",
        "\n",
        "    joblib.dump(model, f'model/{name.replace(\" \", \"_\")}.pkl')\n",
        "    print(f\"✔ {name} completed.\")\n",
        "\n",
        "# 5. Output for README\n",
        "print(\"\\n=== Copy this table to your README ===\")\n",
        "print(pd.DataFrame(results).round(3).to_markdown(index=False))"
      ]
    }
  ]
}